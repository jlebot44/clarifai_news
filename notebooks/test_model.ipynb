{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce08f2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\dev\\tools\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\dev\\tools\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\lebot\\.cache\\huggingface\\hub\\models--roberta-base-openai-detector. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of the model checkpoint at roberta-base-openai-detector were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model_name = \"roberta-base-openai-detector\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "def detect_ai(text: str):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    scores = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    return {\"real\": float(scores[0][0]), \"fake\": float(scores[0][1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c9a9181",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"Un √©l√®ve suspect√© d'avoir frapp√© √† coups de couteau une surveillante d'un coll√®ge √† Nogent (Haute-Marne), alors que des gendarmes proc√©daient √† un contr√¥le des sacs aux abords de l'√©tablissement, a √©t√© arr√™t√© et plac√© en garde √† vue mardi 10 juin, a appris France T√©l√©visions aupr√®s de la gendarmerie. La pr√©fecture de Haute-Marne a annonc√© que l'adolescent a 'bless√© gri√®vement une assistante d'√©ducation' et pr√©cise que la victime √¢g√©e de 31 ans est en 'urgence absolue'.  Elle est actuellement prise en charge par le Samu, sur place. Un gendarme a √©t√© l√©g√®rement bless√© par le couteau au moment de l'interpellation du suspect, pr√©cisent les gendarmes √† France T√©l√©visions. Les 324 √©l√®ves de l'√©tablissement ont √©t√© confin√©s, ajoute la pr√©fecture. La ministre de l'Education nationale Elisabeth Borne et la pr√©f√®te de Haute-Marne ont annonc√© se rendre sur place. J'exprime tout mon soutien √† la victime et √† ses proches, √©crit Elisabeth Borne sur X(Nouvelle fen√™tre)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e044dc88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'real': 0.00018289859872311354, 'fake': 0.9998170733451843}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect_ai(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "547b00a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "import torch\n",
    "import math\n",
    "\n",
    "# üß† Utilise un mod√®le GPT-2 francophone\n",
    "model_name = \"asi/gpt-fr-cased-small\"  # version all√©g√©e de GPT2 pour fran√ßais\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "def compute_perplexity(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "    perplexity = math.exp(loss.item())\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2c1e237",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "27.519954659293212"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_perplexity(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b66762",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\dev\\tools\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement de asi/gpt-fr-cased-small‚Ä¶\n",
      "Chargement de asi/gpt-fr-cased-base‚Ä¶\n",
      "Chargement de dbddv01/gpt2-french-small‚Ä¶\n",
      "Chargement de ClassCat/gpt2-base-french‚Ä¶\n",
      "Chargement de antoiloui/belgpt2‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\dev\\tools\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\lebot\\.cache\\huggingface\\hub\\models--antoiloui--belgpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">perplexit√©</th>\n",
       "      <th colspan=\"2\" halign=\"left\">dur√©e_ms</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texte</th>\n",
       "      <th>humain</th>\n",
       "      <th>ia</th>\n",
       "      <th>humain</th>\n",
       "      <th>ia</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mod√®le</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ClassCat/gpt2-base-french</th>\n",
       "      <td>6.839248</td>\n",
       "      <td>27.356639</td>\n",
       "      <td>161.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>antoiloui/belgpt2</th>\n",
       "      <td>112.966284</td>\n",
       "      <td>31.720293</td>\n",
       "      <td>168.0</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asi/gpt-fr-cased-base</th>\n",
       "      <td>8.083413</td>\n",
       "      <td>18.633750</td>\n",
       "      <td>993.0</td>\n",
       "      <td>337.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asi/gpt-fr-cased-small</th>\n",
       "      <td>9.382521</td>\n",
       "      <td>33.789664</td>\n",
       "      <td>131.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dbddv01/gpt2-french-small</th>\n",
       "      <td>16.381933</td>\n",
       "      <td>83.377777</td>\n",
       "      <td>2800.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           perplexit√©            dur√©e_ms       \n",
       "texte                          humain         ia   humain     ia\n",
       "mod√®le                                                          \n",
       "ClassCat/gpt2-base-french    6.839248  27.356639    161.0   50.0\n",
       "antoiloui/belgpt2          112.966284  31.720293    168.0   48.0\n",
       "asi/gpt-fr-cased-base        8.083413  18.633750    993.0  337.0\n",
       "asi/gpt-fr-cased-small       9.382521  33.789664    131.0   50.0\n",
       "dbddv01/gpt2-french-small   16.381933  83.377777   2800.0   50.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "import torch, math, time\n",
    "import pandas as pd\n",
    "\n",
    "# Cell 2 ‚Äì Mod√®les √† tester\n",
    "model_names = [\n",
    "    \"asi/gpt-fr-cased-small\",\n",
    "    \"asi/gpt-fr-cased-base\",\n",
    "    \"dbddv01/gpt2-french-small\",\n",
    "    \"ClassCat/gpt2-base-french\",\n",
    "    \"antoiloui/belgpt2\"\n",
    "]\n",
    "models = {}\n",
    "\n",
    "# Cell 3 ‚Äì Chargement des mod√®les\n",
    "for name in model_names:\n",
    "    print(f\"Chargement de {name}‚Ä¶\")\n",
    "    tokenizer = GPT2TokenizerFast.from_pretrained(name)\n",
    "    model = GPT2LMHeadModel.from_pretrained(name)\n",
    "    model.eval()\n",
    "    models[name] = (tokenizer, model)\n",
    "\n",
    "# Cell 4 ‚Äì Fonction de perplexit√©\n",
    "def compute_perplexity(tokenizer, model, text, max_length=1024):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss.item()\n",
    "    return math.exp(loss)\n",
    "\n",
    "# Cell 5 ‚Äì Textes d‚Äôexemple\n",
    "texte_humain = \"Aujourd'hui, les √©l√®ves sont all√©s au mus√©e pour d√©couvrir l'histoire de l'art moderne.\"\n",
    "texte_ia = \"La plan√®te est un syst√®me complexe o√π les interactions entre les √©l√©ments naturels cr√©ent des dynamiques √©volutives permanentes.\"\n",
    "\n",
    "# Cell 6 ‚Äì Calcul des perplexit√©s\n",
    "results = []\n",
    "for name, (tok, mod) in models.items():\n",
    "    for label, txt in [(\"humain\", texte_humain), (\"ia\", texte_ia)]:\n",
    "        start = time.time()\n",
    "        ppl = compute_perplexity(tok, mod, txt)\n",
    "        elapsed = (time.time() - start) * 1000\n",
    "        results.append({\n",
    "            \"mod√®le\": name,\n",
    "            \"texte\": label,\n",
    "            \"perplexit√©\": ppl,\n",
    "            \"dur√©e_ms\": int(elapsed)\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "pivot = df.pivot(index=\"mod√®le\", columns=\"texte\", values=[\"perplexit√©\", \"dur√©e_ms\"])\n",
    "pivot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f63e453",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
