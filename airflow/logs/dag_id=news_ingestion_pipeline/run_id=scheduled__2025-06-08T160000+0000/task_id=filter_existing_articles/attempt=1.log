[2025-06-08T19:50:32.905+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: news_ingestion_pipeline.filter_existing_articles scheduled__2025-06-08T16:00:00+00:00 [queued]>
[2025-06-08T19:50:32.912+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: news_ingestion_pipeline.filter_existing_articles scheduled__2025-06-08T16:00:00+00:00 [queued]>
[2025-06-08T19:50:32.913+0000] {taskinstance.py:2170} INFO - Starting attempt 1 of 2
[2025-06-08T19:50:32.925+0000] {taskinstance.py:2191} INFO - Executing <Task(PythonOperator): filter_existing_articles> on 2025-06-08 16:00:00+00:00
[2025-06-08T19:50:32.931+0000] {standard_task_runner.py:60} INFO - Started process 114 to run task
[2025-06-08T19:50:32.935+0000] {standard_task_runner.py:87} INFO - Running: ['***', 'tasks', 'run', 'news_ingestion_pipeline', 'filter_existing_articles', 'scheduled__2025-06-08T16:00:00+00:00', '--job-id', '78', '--raw', '--subdir', 'DAGS_FOLDER/rss_scrap_dag.py', '--cfg-path', '/tmp/tmpp44kwhyi']
[2025-06-08T19:50:32.938+0000] {standard_task_runner.py:88} INFO - Job 78: Subtask filter_existing_articles
[2025-06-08T19:50:32.994+0000] {task_command.py:423} INFO - Running <TaskInstance: news_ingestion_pipeline.filter_existing_articles scheduled__2025-06-08T16:00:00+00:00 [running]> on host 44e96206a18a
[2025-06-08T19:50:33.059+0000] {taskinstance.py:2480} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='news_ingestion_pipeline' AIRFLOW_CTX_TASK_ID='filter_existing_articles' AIRFLOW_CTX_EXECUTION_DATE='2025-06-08T16:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-06-08T16:00:00+00:00'
[2025-06-08T19:50:33.520+0000] {taskinstance.py:2698} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/rss_scrap_dag.py", line 46, in step_2_filter_existing
    existing_ids = filter_existing_articles(uniques, project, dataset, table)
  File "/opt/airflow/src/ingestion/rss_filter.py", line 34, in filter_existing_articles
    existing_ids = get_existing_ids_from_bigquery(project_id, dataset_id, table_id, all_ids)
  File "/opt/airflow/src/ingestion/rss_filter.py", line 22, in get_existing_ids_from_bigquery
    query_job = client.query(query)
  File "/home/airflow/.local/lib/python3.10/site-packages/google/cloud/bigquery/client.py", line 3387, in query
    return _job_helpers.query_jobs_insert(
  File "/home/airflow/.local/lib/python3.10/site-packages/google/cloud/bigquery/_job_helpers.py", line 158, in query_jobs_insert
    future = do_query()
  File "/home/airflow/.local/lib/python3.10/site-packages/google/cloud/bigquery/_job_helpers.py", line 135, in do_query
    query_job._begin(retry=retry, timeout=timeout)
  File "/home/airflow/.local/lib/python3.10/site-packages/google/cloud/bigquery/job/query.py", line 1379, in _begin
    super(QueryJob, self)._begin(client=client, retry=retry, timeout=timeout)
  File "/home/airflow/.local/lib/python3.10/site-packages/google/cloud/bigquery/job/base.py", line 740, in _begin
    api_response = client._call_api(
  File "/home/airflow/.local/lib/python3.10/site-packages/google/cloud/bigquery/client.py", line 818, in _call_api
    return call()
  File "/home/airflow/.local/lib/python3.10/site-packages/google/api_core/retry.py", line 372, in retry_wrapped_func
    return retry_target(
  File "/home/airflow/.local/lib/python3.10/site-packages/google/api_core/retry.py", line 207, in retry_target
    result = target()
  File "/home/airflow/.local/lib/python3.10/site-packages/google/cloud/_http/__init__.py", line 494, in api_request
    raise exceptions.from_http_response(response)
google.api_core.exceptions.BadRequest: 400 POST https://bigquery.googleapis.com/bigquery/v2/projects/ton-projet/jobs?prettyPrint=false: ProjectId must be non-empty

Location: None
Job ID: dedd452c-ac52-4e4e-b4fe-0d1eff6bdf2e

[2025-06-08T19:50:33.540+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=news_ingestion_pipeline, task_id=filter_existing_articles, execution_date=20250608T160000, start_date=20250608T195032, end_date=20250608T195033
[2025-06-08T19:50:33.553+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 78 for task filter_existing_articles (400 POST https://bigquery.googleapis.com/bigquery/v2/projects/ton-projet/jobs?prettyPrint=false: ProjectId must be non-empty

Location: None
Job ID: dedd452c-ac52-4e4e-b4fe-0d1eff6bdf2e
; 114)
[2025-06-08T19:50:33.592+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2025-06-08T19:50:33.615+0000] {taskinstance.py:3280} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-06-08T19:52:59.499+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: news_ingestion_pipeline.filter_existing_articles scheduled__2025-06-08T16:00:00+00:00 [queued]>
[2025-06-08T19:52:59.505+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: news_ingestion_pipeline.filter_existing_articles scheduled__2025-06-08T16:00:00+00:00 [queued]>
[2025-06-08T19:52:59.506+0000] {taskinstance.py:2170} INFO - Starting attempt 1 of 2
[2025-06-08T19:52:59.518+0000] {taskinstance.py:2191} INFO - Executing <Task(PythonOperator): filter_existing_articles> on 2025-06-08 16:00:00+00:00
[2025-06-08T19:52:59.524+0000] {standard_task_runner.py:60} INFO - Started process 129 to run task
[2025-06-08T19:52:59.529+0000] {standard_task_runner.py:87} INFO - Running: ['***', 'tasks', 'run', 'news_ingestion_pipeline', 'filter_existing_articles', 'scheduled__2025-06-08T16:00:00+00:00', '--job-id', '83', '--raw', '--subdir', 'DAGS_FOLDER/rss_scrap_dag.py', '--cfg-path', '/tmp/tmpnss4d4ba']
[2025-06-08T19:52:59.534+0000] {standard_task_runner.py:88} INFO - Job 83: Subtask filter_existing_articles
[2025-06-08T19:52:59.621+0000] {task_command.py:423} INFO - Running <TaskInstance: news_ingestion_pipeline.filter_existing_articles scheduled__2025-06-08T16:00:00+00:00 [running]> on host 44e96206a18a
[2025-06-08T19:52:59.735+0000] {taskinstance.py:2480} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='news_ingestion_pipeline' AIRFLOW_CTX_TASK_ID='filter_existing_articles' AIRFLOW_CTX_EXECUTION_DATE='2025-06-08T16:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-06-08T16:00:00+00:00'
[2025-06-08T19:53:01.078+0000] {python.py:201} INFO - Done. Returned value was: None
[2025-06-08T19:53:01.087+0000] {taskinstance.py:1138} INFO - Marking task as SUCCESS. dag_id=news_ingestion_pipeline, task_id=filter_existing_articles, execution_date=20250608T160000, start_date=20250608T195259, end_date=20250608T195301
[2025-06-08T19:53:01.146+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 0
[2025-06-08T19:53:01.170+0000] {taskinstance.py:3280} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-06-08T19:54:33.150+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: news_ingestion_pipeline.filter_existing_articles scheduled__2025-06-08T16:00:00+00:00 [queued]>
[2025-06-08T19:54:33.156+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: news_ingestion_pipeline.filter_existing_articles scheduled__2025-06-08T16:00:00+00:00 [queued]>
[2025-06-08T19:54:33.157+0000] {taskinstance.py:2170} INFO - Starting attempt 1 of 2
[2025-06-08T19:54:33.169+0000] {taskinstance.py:2191} INFO - Executing <Task(PythonOperator): filter_existing_articles> on 2025-06-08 16:00:00+00:00
[2025-06-08T19:54:33.175+0000] {standard_task_runner.py:60} INFO - Started process 144 to run task
[2025-06-08T19:54:33.179+0000] {standard_task_runner.py:87} INFO - Running: ['***', 'tasks', 'run', 'news_ingestion_pipeline', 'filter_existing_articles', 'scheduled__2025-06-08T16:00:00+00:00', '--job-id', '88', '--raw', '--subdir', 'DAGS_FOLDER/rss_scrap_dag.py', '--cfg-path', '/tmp/tmpbvl6_ll1']
[2025-06-08T19:54:33.183+0000] {standard_task_runner.py:88} INFO - Job 88: Subtask filter_existing_articles
[2025-06-08T19:54:33.239+0000] {task_command.py:423} INFO - Running <TaskInstance: news_ingestion_pipeline.filter_existing_articles scheduled__2025-06-08T16:00:00+00:00 [running]> on host 44e96206a18a
[2025-06-08T19:54:33.304+0000] {taskinstance.py:2480} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='news_ingestion_pipeline' AIRFLOW_CTX_TASK_ID='filter_existing_articles' AIRFLOW_CTX_EXECUTION_DATE='2025-06-08T16:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-06-08T16:00:00+00:00'
[2025-06-08T19:54:34.651+0000] {python.py:201} INFO - Done. Returned value was: None
[2025-06-08T19:54:34.660+0000] {taskinstance.py:1138} INFO - Marking task as SUCCESS. dag_id=news_ingestion_pipeline, task_id=filter_existing_articles, execution_date=20250608T160000, start_date=20250608T195433, end_date=20250608T195434
[2025-06-08T19:54:34.717+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 0
[2025-06-08T19:54:34.740+0000] {taskinstance.py:3280} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-06-08T19:56:27.613+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: news_ingestion_pipeline.filter_existing_articles scheduled__2025-06-08T16:00:00+00:00 [queued]>
[2025-06-08T19:56:27.619+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: news_ingestion_pipeline.filter_existing_articles scheduled__2025-06-08T16:00:00+00:00 [queued]>
[2025-06-08T19:56:27.620+0000] {taskinstance.py:2170} INFO - Starting attempt 1 of 2
[2025-06-08T19:56:27.633+0000] {taskinstance.py:2191} INFO - Executing <Task(PythonOperator): filter_existing_articles> on 2025-06-08 16:00:00+00:00
[2025-06-08T19:56:27.638+0000] {standard_task_runner.py:60} INFO - Started process 165 to run task
[2025-06-08T19:56:27.642+0000] {standard_task_runner.py:87} INFO - Running: ['***', 'tasks', 'run', 'news_ingestion_pipeline', 'filter_existing_articles', 'scheduled__2025-06-08T16:00:00+00:00', '--job-id', '95', '--raw', '--subdir', 'DAGS_FOLDER/rss_scrap_dag.py', '--cfg-path', '/tmp/tmpj550gyzb']
[2025-06-08T19:56:27.646+0000] {standard_task_runner.py:88} INFO - Job 95: Subtask filter_existing_articles
[2025-06-08T19:56:27.701+0000] {task_command.py:423} INFO - Running <TaskInstance: news_ingestion_pipeline.filter_existing_articles scheduled__2025-06-08T16:00:00+00:00 [running]> on host 44e96206a18a
[2025-06-08T19:56:27.767+0000] {taskinstance.py:2480} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='news_ingestion_pipeline' AIRFLOW_CTX_TASK_ID='filter_existing_articles' AIRFLOW_CTX_EXECUTION_DATE='2025-06-08T16:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-06-08T16:00:00+00:00'
[2025-06-08T19:56:28.594+0000] {python.py:201} INFO - Done. Returned value was: None
[2025-06-08T19:56:28.603+0000] {taskinstance.py:1138} INFO - Marking task as SUCCESS. dag_id=news_ingestion_pipeline, task_id=filter_existing_articles, execution_date=20250608T160000, start_date=20250608T195627, end_date=20250608T195628
[2025-06-08T19:56:28.657+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 0
[2025-06-08T19:56:28.681+0000] {taskinstance.py:3280} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-06-08T19:56:59.037+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: news_ingestion_pipeline.filter_existing_articles scheduled__2025-06-08T16:00:00+00:00 [queued]>
[2025-06-08T19:56:59.046+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: news_ingestion_pipeline.filter_existing_articles scheduled__2025-06-08T16:00:00+00:00 [queued]>
[2025-06-08T19:56:59.046+0000] {taskinstance.py:2170} INFO - Starting attempt 1 of 2
[2025-06-08T19:56:59.059+0000] {taskinstance.py:2191} INFO - Executing <Task(PythonOperator): filter_existing_articles> on 2025-06-08 16:00:00+00:00
[2025-06-08T19:56:59.065+0000] {standard_task_runner.py:60} INFO - Started process 177 to run task
[2025-06-08T19:56:59.069+0000] {standard_task_runner.py:87} INFO - Running: ['***', 'tasks', 'run', 'news_ingestion_pipeline', 'filter_existing_articles', 'scheduled__2025-06-08T16:00:00+00:00', '--job-id', '99', '--raw', '--subdir', 'DAGS_FOLDER/rss_scrap_dag.py', '--cfg-path', '/tmp/tmp0izlxbun']
[2025-06-08T19:56:59.073+0000] {standard_task_runner.py:88} INFO - Job 99: Subtask filter_existing_articles
[2025-06-08T19:56:59.133+0000] {task_command.py:423} INFO - Running <TaskInstance: news_ingestion_pipeline.filter_existing_articles scheduled__2025-06-08T16:00:00+00:00 [running]> on host 44e96206a18a
[2025-06-08T19:56:59.197+0000] {taskinstance.py:2480} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='news_ingestion_pipeline' AIRFLOW_CTX_TASK_ID='filter_existing_articles' AIRFLOW_CTX_EXECUTION_DATE='2025-06-08T16:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-06-08T16:00:00+00:00'
[2025-06-08T19:57:00.191+0000] {python.py:201} INFO - Done. Returned value was: None
[2025-06-08T19:57:00.201+0000] {taskinstance.py:1138} INFO - Marking task as SUCCESS. dag_id=news_ingestion_pipeline, task_id=filter_existing_articles, execution_date=20250608T160000, start_date=20250608T195659, end_date=20250608T195700
[2025-06-08T19:57:00.245+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 0
[2025-06-08T19:57:00.269+0000] {taskinstance.py:3280} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-06-08T19:57:38.478+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: news_ingestion_pipeline.filter_existing_articles scheduled__2025-06-08T16:00:00+00:00 [queued]>
[2025-06-08T19:57:38.486+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: news_ingestion_pipeline.filter_existing_articles scheduled__2025-06-08T16:00:00+00:00 [queued]>
[2025-06-08T19:57:38.486+0000] {taskinstance.py:2170} INFO - Starting attempt 1 of 2
[2025-06-08T19:57:38.499+0000] {taskinstance.py:2191} INFO - Executing <Task(PythonOperator): filter_existing_articles> on 2025-06-08 16:00:00+00:00
[2025-06-08T19:57:38.505+0000] {standard_task_runner.py:60} INFO - Started process 192 to run task
[2025-06-08T19:57:38.510+0000] {standard_task_runner.py:87} INFO - Running: ['***', 'tasks', 'run', 'news_ingestion_pipeline', 'filter_existing_articles', 'scheduled__2025-06-08T16:00:00+00:00', '--job-id', '104', '--raw', '--subdir', 'DAGS_FOLDER/rss_scrap_dag.py', '--cfg-path', '/tmp/tmpaek2sc0z']
[2025-06-08T19:57:38.514+0000] {standard_task_runner.py:88} INFO - Job 104: Subtask filter_existing_articles
[2025-06-08T19:57:38.574+0000] {task_command.py:423} INFO - Running <TaskInstance: news_ingestion_pipeline.filter_existing_articles scheduled__2025-06-08T16:00:00+00:00 [running]> on host 44e96206a18a
[2025-06-08T19:57:38.643+0000] {taskinstance.py:2480} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='news_ingestion_pipeline' AIRFLOW_CTX_TASK_ID='filter_existing_articles' AIRFLOW_CTX_EXECUTION_DATE='2025-06-08T16:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-06-08T16:00:00+00:00'
[2025-06-08T19:57:39.502+0000] {python.py:201} INFO - Done. Returned value was: None
[2025-06-08T19:57:39.511+0000] {taskinstance.py:1138} INFO - Marking task as SUCCESS. dag_id=news_ingestion_pipeline, task_id=filter_existing_articles, execution_date=20250608T160000, start_date=20250608T195738, end_date=20250608T195739
[2025-06-08T19:57:39.564+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 0
[2025-06-08T19:57:39.586+0000] {taskinstance.py:3280} INFO - 1 downstream tasks scheduled from follow-on schedule check
